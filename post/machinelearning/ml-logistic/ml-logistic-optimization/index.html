<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.css">
  <script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.liushulun.cn","root":"/","images":"/images","scheme":"Gemini","version":"8.3.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#EB6D39","save":"manual"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeInDown","post_header":null,"post_body":null,"coll_header":"none","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="（1）牛顿法和拟牛顿法；（2）Logistic 回归的优化求解。">
<meta property="og:type" content="article">
<meta property="og:title" content="ML入门-Logistic优化求解与牛顿法">
<meta property="og:url" content="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/index.html">
<meta property="og:site_name" content="Luis&#39;s Blogs">
<meta property="og:description" content="（1）牛顿法和拟牛顿法；（2）Logistic 回归的优化求解。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/newton_iteration.png">
<meta property="og:image" content="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/double_circle_direction.png">
<meta property="article:published_time" content="2020-01-10T03:02:39.000Z">
<meta property="article:author" content="Luis">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="MachineLearning">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Logistic回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/newton_iteration.png">


<link rel="canonical" href="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

<title>ML入门-Logistic优化求解与牛顿法 | Luis's Blogs</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle Navigation" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Luis's Blogs</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">V 1.1.5</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">62</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">86</span></a></li>
        
            
  <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-comments fa-fw"></i>About</a></li>


      
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>


</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Catalogue
        </li>
        <li class="sidebar-nav-overview">
          SiteInfo
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ML%E5%85%A5%E9%97%A8-Logistic%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E4%B8%8E%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-text">ML入门-Logistic优化求解与牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-text">1. 牛顿法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-text">2. 拟牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-BFGS"><span class="nav-text">2.1 BFGS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-L-BFGS"><span class="nav-text">2.2 L-BFGS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Logistic%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95"><span class="nav-text">3. Logistic优化求解算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%A2%AF%E5%BA%A6"><span class="nav-text">3.1 梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Hessian%E7%9F%A9%E9%98%B5"><span class="nav-text">3.2 Hessian矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E7%89%9B%E9%A1%BF%E6%B3%95%E6%B1%82%E8%A7%A3Logistic%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%9E%81%E5%B0%8F%E5%80%BC%EF%BC%9AIRLS"><span class="nav-text">3.3 牛顿法求解Logistic损失函数和极小值：IRLS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Logistic%E7%9A%84%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E5%99%A8Solver"><span class="nav-text">4. Logistic的优化求解器Solver</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Luis"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Luis</p>
  <div class="site-description" itemprop="description">Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">Posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">Categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">86</span>
        <span class="site-state-item-name">Tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/liushulun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liushulun" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:luis96@foxmail.com" title="E-Mail → mailto:luis96@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/about/" title="Chats → &#x2F;about&#x2F;"><i class="fa fa-comments fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/" title="Home → &#x2F;"><i class="fa fa-home fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-users fa-fw"></i>
      友链
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://renxingkai.github.io/" title="https:&#x2F;&#x2F;renxingkai.github.io" rel="noopener" target="_blank">CinKate</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.juanertu.com/" title="https:&#x2F;&#x2F;blog.juanertu.com" rel="noopener" target="_blank">ConstOwn</a>
        </li>
    </ul>
  </div>


        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/liushulun" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Luis">
      <meta itemprop="description" content="Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luis's Blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML入门-Logistic优化求解与牛顿法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted:</span>

      <time title="Created::2020-01-10 11:02:39" itemprop="dateCreated datePublished" datetime="2020-01-10T11:02:39+08:00">2020-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">Categories:</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a>
        </span>
          ,
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/LogisticRegression/" itemprop="url" rel="index"><span itemprop="name">LogisticRegression</span></a>
        </span>
    </span>

  






<!-- 注释开始 #############################################

  <script>
    function post_nav(post) {
      const theme = hexo.theme.config;
      if (theme.post_navigation === false || (!post.prev && !post.next)) return '';
      const prev = theme.post_navigation === 'right' ? post.prev : post.next;
      const next = theme.post_navigation === 'right' ? post.next : post.prev;
      const left = prev ? `
        <a href="${this.url_for(prev.path)}" rel="prev" title="${prev.title}">
          <i class="fa fa-chevron-left"></i> ${prev.title}
        </a>` : '';
      const right = next ? `
        <a href="${this.url_for(next.path)}" rel="next" title="${next.title}">
          ${next.title} <i class="fa fa-chevron-right"></i>
        </a>` : '';
      return `
        <div class="post-nav">
          <div class="post-nav-item">${left}</div>
          <div class="post-nav-item">${right}</div>
        </div>`;
    }
  </script>
  
  <div style="position: absolute; top: -20px;">post_nav(post)</div>

############################################# 注释结束 -->



<!-- 注释开始 #############################################
  
  <div style="position: fixed; width: 100%;">
    
      
      <div class="post-tags">
          <a href="/tags/AI/" rel="tag"><i class="fa fa-tag"></i> AI</a>
          <a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a>
          <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
          <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          <a href="/tags/Logistic%E5%9B%9E%E5%BD%92/" rel="tag"><i class="fa fa-tag"></i> Logistic回归</a>
      </div>
  </div>
############################################# 注释结束 -->

    <span class="post-meta-item" title="Post Symbols">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Post Symbols:</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="Read Time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Read Time &asymp;</span>
      <span>17 Min</span>
    </span>
</div>

            <div class="post-description">（1）牛顿法和拟牛顿法；（2）Logistic 回归的优化求解。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <span id="more"></span>

<h1 id="ML入门-Logistic优化求解与牛顿法"><a href="#ML入门-Logistic优化求解与牛顿法" class="headerlink" title="ML入门-Logistic优化求解与牛顿法"></a>ML入门-Logistic优化求解与牛顿法</h1><p>前言：在机器学习的优化求解算法中，梯度下降法是无约束优化问题优化求解最常用的方法之一，还有一种求解方法就是牛顿法。相比梯度下降法，牛顿法的收敛速度更快，但同时，每次迭代需要的计算量也更大。</p>
<p>在牛顿法之前，需要先了解：<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/泰勒公式/7681487?fr=aladdin" title="@LINK">泰勒公式</a>。泰勒公式通俗地讲，就是当函数 $f$ 在点 $x$ 处的一阶导数、二阶导数……n 阶导数已知时，即可使用 n 阶泰勒展开来逼近函数 $f$ 在点 $x$ 的 <strong>邻域</strong> 的函数值，因此泰勒公式求的是一个点的邻域的近似函数值。</p>
<p>对应优化求解中，通常我们习惯于使用迭代法来求解，而迭代法的本质即：每次向正确的方向移动一小段，直到达到给定条件。这就与泰勒公式不谋而合——每次移动一小段之后的函数值可以使用移动前该点的泰勒展开来逼近，因此牛顿法或拟牛顿法都是在泰勒公式的基础上进行的。</p>
<hr>
<h2 id="1-牛顿法"><a href="#1-牛顿法" class="headerlink" title="1. 牛顿法"></a>1. 牛顿法</h2><p>牛顿法（Newton - Raphson，牛顿 - 拉夫逊）是牛顿在 17 世纪提出的用于求解方程的根的方法。其求解思想如下：</p>
<ol>
<li>假设点 $x^{\ast}$ 为函数 $f(x)$ 的根，则 $f(x) = 0$。</li>
<li>将函数 $f(x)$ 在点 $x_0$ 处进行一阶泰勒展开有：$f(x) \approx f(x_0) + (x - x_0) f’(x_0)$</li>
<li>假设点 $x$ 为 $x_0$ 邻域内一点，且 $x$ 为函数的根，则有：$f(x) \approx f(x_0) + (x - x_0) f’(x_0) = 0$</li>
<li>将上式变换即可得：$x = x_0 - \dfrac {f(x_0)} {f’(x_0)}$</li>
</ol>
<p>上述牛顿法得到的结论，拓展到迭代的过程中，假设当前处在迭代第 t 轮，则可以得到下一轮 (t + 1) 时刻的解的表达式为：</p>
<p>$<br>x^{(t + 1)} = x^{(t)} - \dfrac {f(x^{(t)})} {f’(x^{(t)})}<br>$</p>
<p>这就是牛顿法优化求解的基本思想。下图展示了牛顿法求解方程 $f(x) = 0$ 的根的过程（图自 Wiki）：</p>
<p><img data-src="./newton_iteration.png" alt="牛顿法迭代过程" title="@ASSET"></p>
<p>回到最优化问题中，通常会将问题转化成求极小值（误差、损失最小等），极小值对应了函数的导数为 0，因此需要适当调整牛顿法的目标，从求 $f(x) = 0$ 变为求 $f’(x) = 0$，因此原问题变为求 $f’(x)$ 的根。</p>
<p>令 $g(x) = f’(x)$，则关于 $x$ 的迭代条件变为：</p>
<p>$<br>x^{(t + 1)} = x^{(t)} - \dfrac {g(x^{(t)})} {g’(x^{(t)})} = x^{(t)} - \dfrac {f’(x^{(t)})} {f’’(x^{(t)})}<br>$</p>
<p>在实际问题中，通常输入 $X$ 的维度都大于 1，因此将一阶导数替换为梯度（即 $f$ 分别对每个 $x_i$ 求偏导后组成向量）：</p>
<p>$<br>\nabla f(x_1, \cdots, x_D)<br>$</p>
<p>将二阶导数替换为海森（Hessian）矩阵 H：</p>
<p>$<br>H(X) = \left[<br>\begin{matrix}<br>\dfrac {\partial^2 f} {\partial^2 x^2_1} &amp; \dfrac {\partial^2 f} {\partial x_1 \partial x_2} &amp; \cdots &amp; \dfrac {\partial^2 f} {\partial x_1 \partial x_D}<br>\\<br>\dfrac {\partial^2 f} {\partial x_2 \partial x_1} &amp; \dfrac {\partial^2 f} {\partial^2 x^2_2} &amp; \cdots &amp; \dfrac {\partial^2 f} {\partial x_2 \partial x_D}<br>\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots<br>\\<br>\dfrac {\partial^2 f} {\partial x_D \partial x_1} &amp; \dfrac {\partial^2 f} {\partial x_D \partial x_2} &amp; \cdots &amp; \dfrac {\partial^2 f} {\partial^2 x^2_D}<br>\end{matrix}<br>\right]<br>$</p>
<p>Hessian 矩阵即：第 i 行第 j 列的元素为 $f$ 先对 $x_i$ 求偏导后再对 $x_j$ 求偏导。由于 $f$ 先对 $x_i$ 再对 $x_j$ 求偏导和 $f$ 先对 $x_j$ 再对 $x_i$ 求偏导相等，即 $\dfrac {\partial^2 f} {\partial x_i \partial x_j} = \dfrac {\partial^2 f} {\partial x_j \partial x_i}$，因此 Hessian 矩阵是对称的。</p>
<p>这样，牛顿法的迭代公式就变换为：</p>
<p>$$<br>x^{(t + 1)} = x^{(t)} - H^{-1} (X^{(t)}) \ \nabla f(X^{(t)})<br>$$</p>
<blockquote>
<p>@注：二阶导转换为 Hessian 矩阵 $H(X^{(t)})$ 后作为分母，使用逆运算 $H^{-1} (X^{(t)})$ 来表示。</p>
</blockquote>
<p>总结牛顿法求解目标函数极值的迭代步骤如下：</p>
<ol>
<li>从 $t = 0$ 开始，初始化 $X^{(0)}$ 为随机值</li>
<li>计算目标函数 $f(X)$ 在点 $X^{(t)}$ 的梯度：$g^{(t)} = \nabla f(X^{(t)})$，以及 Hessian 矩阵：$H^{(t)} = H(X^{(t)})$</li>
<li>计算移动方向：$d^{(t)} = (H^{(t)})^{-1} \ g^{(t)}$</li>
<li>根据迭代公式更新 $X$ 的值：$X^{(t + 1)} = X^{(t)} - d^{(t)}$</li>
<li>判断是否满足迭代终止条件（是否到达最大迭代次数，或相邻两次迭代的相对变化量或绝对变化量小于预设值，通常使用绝对量：$\dfrac {f(X^{(t + 1)}) - f(X^{(t)})} {f(X^{(t)})} \le \varepsilon$），若满足则循环计数，返回最佳参数 $X^{(t + 1)}$ 和目标函数极小值 $f(X^{(t + 1)})$，否则跳转到第 2 步</li>
</ol>
<p>其中，第 3 步计算移动方向 $d^{(t)}$ 时，由于矩阵的逆求解困难，因此常用线性方程组计算：$H^{(t)} d^{(t)} = g^{(t)}$，当 $X$ 维度比较小时，可采用解析法求解 $d^{(t)}$，当 $X$ 维度比较高时，可采用梯度下降法或共而梯度下降法求解，因此对 $d^{(t)}$ 的求解又是一个迭代的计算过程。</p>
<p>对比梯度下降法中的移动方向：$d^{(t)} = - \eta g^{(t)}$，牛顿法：$d^{(t)} = - (H^{(t)})^{-1} \ g^{(t)}$，Hessian 矩阵相比学习率（步长）$\eta$ 包含的信息更多，因此牛顿法收敛速度更快，但从上述步骤也可明显看出牛顿法每次迭代的计算量都大幅增加。</p>
<p>由于梯度下降法仅使用了一阶导数，而牛顿法使用了二阶导数矩阵，因此梯度下降法是一阶最优化算法，而牛顿法是二阶最优化算法。</p>
<hr>
<h2 id="2-拟牛顿法"><a href="#2-拟牛顿法" class="headerlink" title="2. 拟牛顿法"></a>2. 拟牛顿法</h2><p>牛顿法虽然收敛速度比梯度下降法更快，但在高维的情况下，计算目标函数二阶偏导数的复杂度很大，而且有时候目标函数的 Hessian 矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。</p>
<p>为此提出：拟牛顿法（Quasi-Newton Methods），拟牛顿法旨在：不用二阶偏导数，而构造出可以近似 Hessian 矩阵（或 Hessian 矩阵的逆矩阵）的正定对称矩阵，再逐步优化目标函数。不同的近似 Hessian 矩阵构造方法产生了不同的拟牛顿法：BFGS / L-BFGS。</p>
<blockquote>
<p>扩展阅读：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/aws3217150/article/details/50548177" title="@LINK">谈谈常见的迭代优化方法</a></li>
<li><a target="_blank" rel="noopener" href="https://www.scipy-lectures.org/advanced/mathematical_optimization/" title="@LINK">Mathematical optimization: finding minima of functions</a></li>
</ol>
</blockquote>
<p>重新考虑迭代条件，假设有某点 $x_0$，目标函数 $f(x)$ 在该点（已知点）进行二阶泰勒展开：</p>
<p>$<br>f(x) \approx f(x_0) + f’(x_0) (x - x_0) + \dfrac {1} {2} f’’(x_0) (x - x_0)^2<br>$</p>
<p>当 $X$ 为 向量 / 矩阵 时，导数即为梯度，上式转换为：</p>
<p>$<br>f(X) \approx f(X_0) + \nabla f(X_0) (X - X_0) + \dfrac {1} {2} (X - X_0)^T \nabla^2 f(X_0) (X - X_0)<br>$</p>
<p>对上式取梯度运算（$X_0$ 为已知点，因此其与其导数均可视为常数项）得：</p>
<p>$<br>\nabla f(X) \approx \nabla f(X_0) + \nabla^2 f(X_0) (X - X_0)<br>$</p>
<p>由于函数在点 $X_0$ 处的泰勒展开可以近似 $X_0$ 邻域内的函数值，假设 $X_0$ 即为迭代 t 次后得到的 $X^{(t + 1)}$，则迭代前一轮的 $X^{(t)}$ 即为邻域内一点，可用二阶泰勒展开逼近，因此可得迭代关系：</p>
<p>$<br>\nabla f(X^{(t)}) \approx \nabla f(X^{(t + 1)}) + \nabla^2 f(X^{(t + 1)}) (X^{(t)} - X^{(t + 1)})<br>$</p>
<p>再使用 gradiant 和 Hessian 分别表示 $X$ 的一阶梯度和二阶梯度矩阵：$g^{(t)} = \nabla f(X^{(t)})$，$H^{(t)} = \nabla^2 f(X^{(t)})$，整理可得：</p>
<p>$<br>g^{(t + 1)} - g^{(t)} \approx H^{(t + 1)} (X^{(t + 1)} - X^{(t)})<br>$</p>
<p>进一步，引入记号：$s^{(t)} = X^{(t + 1)} - X^{(t)}$ 表示 $X$ 的变化量，$y^{(t)} = g^{(t + 1)} - g^{(t)}$ 表示梯度变化量，则可得简洁迭代关系：</p>
<p>$$<br>y^{(t)} \approx H^{(t + 1)} s^{(t)}<br>$$</p>
<p>由于牛顿法中 Hessian 矩阵的逆难以计算，因此在拟牛顿法中，令 $B$ 表示 $H$ 的近似，$D$ 表示 $H^{-1}$ 的近似，代入上式即可得到 <strong>拟牛顿法的条件&lt;</strong> 为：</p>
<p>$$<br>y^{(t)} = B^{(t + 1)} s^{(t)}<br>$$</p>
<p>或</p>
<p>$$<br>s^{(t)} = D^{(t + 1)} y^{(t)}<br>$$</p>
<p><strong>实际上，拟牛顿法的条件给出了 Hessian 矩阵的近似需要满足的条件。</strong></p>
<h3 id="2-1-BFGS"><a href="#2-1-BFGS" class="headerlink" title="2.1 BFGS"></a>2.1 BFGS</h3><p>BFGS（Broyden, Fletcher, Glodfarb, Shanno）被认为是数值效果最好的拟牛顿法，且具有全局收敛性和超线性收敛速度。</p>
<p>BFGS 算法采用迭代法逼近 Hessian 矩阵：$B^{(t + 1)} = B^{(t)} + \Delta B^{(t)}$，初始值 $B^{(0)} = I$ 为单位矩阵，因此关键在于如何构造 $\Delta B^{(t)}$。</p>
<p>为保证矩阵 $B$ 的正定性，令 $\Delta B^{(t)} = \alpha u u^T + \beta v v^T$，代入上述拟牛顿法条件可得：</p>
<p>$<br>\begin{aligned}<br>y^{(t)} &amp;= B^{(t + 1)} s^{(t)} = (B^{(t)} + \Delta B^{(t)}) s^{(t)} = B^{(t)} s^{(t)} + \Delta B^{(t)} s^{(t)}<br>\\<br>&amp;= B^{(t)} s^{(t)} + \alpha u u^T s^{(t)} + \beta v v^T s^{(t)}<br>\\<br>&amp;= B^{(t)} s^{(t)} + u (\alpha u^T s^{(t)}) + v (\beta v^T s^{(t)})<br>\end{aligned}<br>$</p>
<blockquote>
<p>@注：当 $u^T$ 或 $v^T$ 的维数与 $s^{(t)}$ 的维数一致（由于 $\Delta B^{(t)}$ 是构造的，因此可以构造为相同维数）时，$u^T s^{(t)}$ 以及 $v^T s^{(t)}$ 均为一个标量数值（向量的转置 x 向量 = 数值）。</p>
</blockquote>
<p>令 $\alpha u^T s^{(t)} = 1$，$\beta v^T s^{(t)} = -1$，即 $\alpha = \dfrac {1} {u^T s^{(t)}}$，$\beta = - \dfrac {1} {v^T s^{(t)}}$，得到：</p>
<p>$<br>u - v = y^{(t)} - B^{(t)} s^{(t)}<br>$</p>
<p>不妨令 $u = y^{(t)}$，$v = B^{(t)} s^{(t)}$，代入 $\alpha$ 和 $\beta$ 的表达式得：</p>
<p>$<br>\alpha = \dfrac {1} {u^T s^{(t)}} = \dfrac {1} {(y^{(t)})^T s^{(t)}}<br>$</p>
<p>$<br>\beta = - \dfrac {1} {v^T s^{(t)}} = - \dfrac {1} {(B^{(t)} s^{(t)})^T s^{(t)}} = - \dfrac {1} {(s^{(t)})^T (B^{(t)})^T s^{(t)}}<br>$</p>
<p><strong>代入 $\Delta B^{(t)}$ 的表达式得：</strong></p>
<p>$$<br>\begin{aligned}<br>\Delta B^{(t)} &amp;= \alpha u u^T + \beta v v^T<br>\\<br>&amp;= \dfrac {y^{(t)} (y^{(t)})^T} {(y^{(t)})^T s^{(t)}} - \dfrac {B^{(t)} s^{(t)} (B^{(t)} s^{(t)})^T} {(s^{(t)})^T (B^{(t)})^T s^{(t)}}<br>\end{aligned}<br>$$</p>
<blockquote>
<p>@注：Sherman-Morrison 公式：若 $A$ 为非奇异方阵，$1 + v^T A^{-1} u \ne 0$，则有：</p>
</blockquote>
<p>$<br>(A + uv^T)^{-1} = A^{-1} - \dfrac {A^{-1} u v^T A^{-1}} {1 + v^T A^{-1}}<br>$</p>
<p><strong>由于牛顿法迭代过程需要计算 Hessian 矩阵的逆矩阵，因此根据 Sherman-Morrison 公式可得：</strong></p>
<p>$$<br>(B^{(t + 1)})^{-1} = D^{(t + 1)} = \left( I - \dfrac {s^{(t)} (y^{(t)})^T} {(y^{(t)})^T s^{(t)}} \right) D^{(t)} \left( I - \dfrac {y^{(t)} (s^{(t)})^T} {(y^{(t)})^T s^{(t)}} \right) + \dfrac {s^{(t)} (s^{(t)})^T} {(y^{(t)})^T s^{(t)}}<br>$$</p>
<p>综上，对于 $\Delta B^{(t)}$ 和 $D^{(t + 1)}$，均可使用 $s^{(t)}$（$X$ 的变化量）和 $y^{(t)}$（函数梯度的变化量）表示，而初始化 $B^{(0)} = D^{(0)} = I$ 是一指的，$s^{(t)}$ 和 $y^{(t)}$ 在 $X$ 已知（优化求解的目的是寻找最优的 $X$，而 $X$ 本来就是已知的）时均是可计算的。</p>
<p>整理得 BFGS 更新参数的流程如下：</p>
<ol>
<li><p>从 $t = 0$ 开始，初始化 $D^{(0)} = I$</p>
</li>
<li><p>计算移动方向：$d^{(t)} = D^{(t)} g^{(t)}$</p>
<blockquote>
<p>@注：先用 $B^{(t)} = B^{(t - 1)} + \Delta B^{(t - 1)}$ 迭代解出 $B^{(t)}$，再用 Sherman-Morrison 公式解出 $D^{(t)}$。</p>
</blockquote>
</li>
<li><p>更新 $X$ 的值：$X^{(t + 1)} = X^{(t)} - d^{(t)}$</p>
</li>
<li><p>$s^{(t)} = d^{(t)}$</p>
</li>
<li><p>若 $||g^{(t + 1)}|| \le \varepsilon$，则迭代终止</p>
</li>
<li><p>计算：$y^{(t)} = g^{(t + 1)} - g^{(t)}$</p>
</li>
<li><p>$t = t + 1$，跳转第 2 步</p>
</li>
</ol>
<h3 id="2-2-L-BFGS"><a href="#2-2-L-BFGS" class="headerlink" title="2.2 L-BFGS"></a>2.2 L-BFGS</h3><p>在 BFGS 中，每一轮迭代需要存储 Hessian 矩阵或其近似（$B^{(t)}$ 或 $D^{(t)}$），但当 $X$ 维数很高时，矩阵的维度也会很高，需要耗费大量存储空间。</p>
<p>L-BFGS（Limited memory BFGS）不直接存储 Hessian 矩阵或其近似（$B^{(t)}$ 或 $D^{(t)}$），而是存储迭代计算过程中的 $s^{(t)}$ 和 $y^{(t)}$ 来计算，从而减少参数存储所需空间。</p>
<p>在 BFGS 中，Hessian 矩阵的更新公式为：</p>
<p>$<br>(B^{(t + 1)})^{-1} = D^{(t + 1)} = \left( I - \dfrac {s^{(t)} (y^{(t)})^T} {(y^{(t)})^T s^{(t)}} \right) D^{(t)} \left( I - \dfrac {y^{(t)} (s^{(t)})^T} {(y^{(t)})^T s^{(t)}} \right) + \dfrac {s^{(t)} (s^{(t)})^T} {(y^{(t)})^T s^{(t)}}<br>$</p>
<p>令 $\rho^{(t)} = \dfrac {1} {(y^{(t)})^T s^{(t)}}$，$V^{(t)} = \left( I - \dfrac {y^{(t)} (s^{(t)})^T} {(y^{(t)})^T s^{(t)}} \right) = I - \rho^{(t)} y^{(t)} (s^{(t)})^T$</p>
<blockquote>
<p>@注：由于向量 $s^{(t)}$（X的变化量）和向量 $y^{(t)}$（梯度变化量）的维数相同（都与 $X$ 维度相等），因此 $s^{(t)} (y^{(t)})^T$ 和 $s^{(t)} (s^{(t)})^T$ 是矩阵，而 $(y^{(t)})^T s^{(t)}$ 是一个数值，因此 $\rho^{(t)}$ 是一个常数。</p>
</blockquote>
<p>则有：</p>
<p>$<br>\begin{aligned}<br>(V^{(t)})^T &amp;= \left( I - \rho^{(t)} y^{(t)} (s^{(t)})^T \right)^T<br>\\<br>&amp;= I - \rho^{(t)} \left( y^{(t)} (s^{(t)})^T \right)^T<br>\\<br>&amp;= I - \rho^{(t)} s^{(t)} (y^{(t)})^T<br>\\<br>&amp;= I - \dfrac {s^{(t)} (y^{(t)})^T} {(y^{(t)})^T s^{(t)}}<br>\end{aligned}<br>$</p>
<p>因此原 Hessian 更新公式变为：</p>
<p>$$<br>D^{(t + 1)} = (V^{(t)})^T D^{(t)} V^{(t)} + \rho^{(t)} s^{(t)} (s^{(t)})^T<br>$$</p>
<p>将上述迭代更新公式展开得：</p>
<p>$<br>\begin{aligned}<br>D^{(t + 1)} &amp;= (V^{(t)})^T D^{(t)} V^{(t)} + \rho^{(t)} s^{(t)} (s^{(t)})^T<br>\\<br>&amp; \Downarrow<br>\\<br>D^{(1)} &amp;= (V^{(0)})^T D^{(0)} V^{(0)} + \rho^{(0)} s^{(0)} (s^{(0)})^T<br>\\ \\<br>D^{(2)} &amp;= (V^{(1)})^T D^{(1)} V^{(1)} + \rho^{(1)} s^{(1)} (s^{(1)})^T<br>\\<br>&amp;= (V^{(1)})^T \left( (V^{(0)})^T D^{(0)} V^{(0)} + \rho^{(0)} s^{(0)} (s^{(0)})^T \right) V^{(1)} + \rho^{(1)} s^{(1)} (s^{(1)})^T<br>\\<br>&amp;= (V^{(1)})^T (V^{(0)})^T D^{(0)} V^{(0)} V^{(1)} + (V^{(1)})^T \rho^{(0)} s^{(0)} (s^{(0)})^T V^{(1)} + \rho^{(1)} s^{(1)} (s^{(1)})^T<br>\\<br>&amp; \ \ \vdots<br>\\<br>D^{(t + 1)} &amp;= \left[ (V^{(t)})^T (V^{(t - 1)})^T \dots (V^{(0)})^T \right] D^{(0)} \left[ V^{(0)} V^{(1)} \dots V^{(t)} \right]<br>\\<br>&amp; \ + \left[ (V^{(t)})^T (V^{(t - 1)})^T \dots (V^{(1)})^T \right] \left[ \rho^{(0)} s^{(0)} (s^{(0)})^T \right] \left[ V^{(1)} V^{(2)} \dots V^{(t)} \right]<br>\\<br>&amp; \ + \left[ (V^{(t)})^T (V^{(t - 1)})^T \dots (V^{(2)})^T \right] \left[ \rho^{(1)} s^{(1)} (s^{(1)})^T \right] \left[ V^{(2)} V^{(3)} \dots V^{(t)} \right]<br>\\<br>&amp; \ + \ \cdots \cdots<br>\\<br>&amp; \ + (V^{(t)})^T \left[ \rho^{(t - 1)} s^{(t - 1)} (s^{(t - 1)})^T \right] V^{(t)}<br>\\<br>&amp; \ + \rho^{(t)} s^{(t)} (s^{(t)})^T<br>\end{aligned}<br>$</p>
<p>从上述迭代过程可知，计算 $D^{(t + 1)}$ 需要用到 $\left \{ s^{(k)} y^{(k)} \right \}^t_{k = 0}$，若存储空间有限，仅能存储 m 组 $\left \{ s^{(k)} y^{(k)} \right \}$，当 $t &gt; m$ 时，应当丢弃较早生成的 $\left \{ s^{(k)} y^{(k)} \right \}$。当然，由于丢弃了部分信息，此时计算的 $D^{(m + 1)}$ 是 $D^{(t + 1)}$ 的近似，也即 Hessian 矩阵的逆矩阵的进一步近似。</p>
<p>当 $t &gt; m + 1$ 时，构造近似公式：</p>
<p>$<br>\begin{aligned}<br>D^{(t + 1)} &amp;= \left[ (V^{(t)})^T (V^{(t - 1)})^T \dots (V^{(t - m + 1)})^T \right] D^{(0)} \left[ V^{(t - m + 1)} V^{(1)} \dots V^{(t)} \right]<br>\\<br>&amp; \ + \left[ (V^{(t)})^T (V^{(t - 1)})^T \dots (V^{(t - m + 2)})^T \right] \left[ \rho^{(0)} s^{(0)} (s^{(0)})^T \right] \left[ V^{(t - m + 2)} V^{(2)} \dots V^{(t)} \right]<br>\\<br>&amp; \ + \ \cdots \cdots<br>\\<br>&amp; \ + (V^{(t)})^T \left[ \rho^{(t - 1)} s^{(t - 1)} (s^{(t - 1)})^T \right] V^{(t)}<br>\\<br>&amp; \ + \rho^{(t)} s^{(t)} (s^{(t)})^T<br>\end{aligned}<br>$</p>
<p>$D^{(t)}$ 的迭代计算很繁琐，但计算 $D^{(t)}$ 的目的是为了得到搜索方向 $d^{(t)} = D^{(t)} g^{(t)}$，因此可以设计快速计算 $D^{(t)} g^{(t)}$ 的方法：</p>
<p><img data-src="./double_circle_direction.png" alt="双向循环快速求解搜索方向" title="@ASSET"></p>
<p>算法中还有部分不太明白，暂时只放上图片，待研究透彻后改为 Python 代码形式。另有几篇关于该双向循环快速求解 $D^{(t)} g^{(t)}$ 算法的参考文章如下：</p>
<blockquote>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/red_stone1/article/details/80821760" title="@LINK">机器学习中牛顿法凸优化的通俗解释（作者：红色石头Will）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/itplus/article/details/21897715" title="@LINK">牛顿法与拟牛顿法学习笔记（五）L-BFGS 算法（作者：皮果提）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/snaillup/article/details/53524415" title="@LINK">spark L-BFGS实现（作者：snaillup）</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/scalanlp/breeze/blob/master/math/src/main/scala/breeze/optimize/LBFGS.scala" title="@LINK">LBFGS.scala（作者：ScalaNLP）</a></li>
<li><a target="_blank" rel="noopener" href="https://liuxiaofei.com.cn/blog/lbfgs方法推导/#lbfgs方法推导" title="@LINK">LBFGS方法推导（作者：慢慢的回味）</a></li>
</ol>
</blockquote>
<hr>
<h2 id="3-Logistic优化求解算法"><a href="#3-Logistic优化求解算法" class="headerlink" title="3. Logistic优化求解算法"></a>3. Logistic优化求解算法</h2><p>Logistic 回归采用 Logistic 损失 / 交叉熵损失：</p>
<p>$<br>L(y, \mu (X)) = - y \log (\mu (X)) - (1 - y) \log (1 - \mu (X))<br>$</p>
<p>其中 $y$ 为真值， $\mu (X)$ 为预测值为 1 的概率。</p>
<p>与其他机器学习一样，Logistic 回归的目标函数也包括两项：训练集上的损失和 + 正则项。</p>
<p>$<br>J(W; \lambda) = \sum^N_{i = 1} L(y_i, \mu (X_i; W)) + \lambda R(W)<br>$</p>
<blockquote>
<p>@注：由于 L1 正则在零点不可导，因此当正则项中含有 L1 正则时，不能直接使用基于梯度、Hessian 矩阵的优化求解算法，而通常使用坐标轴下降法求解，或也可使用次梯度法。</p>
</blockquote>
<p>在给定正则参数 $\lambda$ 的情况下，目标函数的最优解为：$\hat{W} = \arg_W \min J(W, \lambda)$，取得最优解的必要条件即一阶导数为零：$\dfrac {\partial J(W, \lambda)} {\partial W} = 0$。</p>
<p>与线性回归模型不同的是，Logistic 回归模型的参数无法用解析法求解，因此可使用迭代法逼近求解。</p>
<p>其中一阶近似有与梯度相关的几个算法：</p>
<ul>
<li>梯度下降（Logistic 使用梯度下降法收敛速度较慢）</li>
<li>随机梯度下降（SGD）</li>
<li>随机平均梯度法（SAG）</li>
<li>随机平均梯度法改进版（SAGA）</li>
<li>共轭梯度</li>
<li>坐标轴下降</li>
</ul>
<p>二阶近似有：</p>
<ul>
<li>牛顿法</li>
<li>拟牛顿法（BFGS、L-BFGS）</li>
</ul>
<p>观察目标函数的损失和部分：</p>
<p>$<br>J_1 (W) = \sum^N_{i = 1} \left( - y_i \log (\mu (X_i; W)) - (1 - y_i) \log (1 - \mu (X_i; W)) \right)<br>$</p>
<h3 id="3-1-梯度"><a href="#3-1-梯度" class="headerlink" title="3.1 梯度"></a>3.1 梯度</h3><p>最优化问题的求解离不开梯度的计算，由于目标函数中包含了 $\mu$，即 Sigmoid 变换，记 $\mu_i = \mu (X_i; W) = \sigma (W^T X_i)$，令 $z_i = W^T X_i$，根据复合函数的求导，$\dfrac {\partial \mu_i} {\partial W}$ 的求解如下：</p>
<p>$<br>\begin{aligned}<br>\dfrac {\partial \mu_i} {\partial W} &amp;= \dfrac {\partial \sigma (W^T X_i)} {\partial W} = \dfrac {\partial \sigma {z_i}} {\partial W}<br>\\<br>&amp;= \dfrac {d \sigma (z_i)} {d z_i} \dfrac {\partial z_i} {\partial W}<br>\end{aligned}<br>$</p>
<p>其中 ① 复合函数外层 $\sigma (z_i)$ 求导：</p>
<p>$<br>\begin{aligned}<br>\dfrac {d \sigma (z_i)} {d z_i} &amp;= \dfrac {d (\dfrac {1} {1 + e^{- z_i}})} {d z_i}<br>\\<br>&amp;= - \dfrac {1} {(1 + e^{- z_i})^2} \times \dfrac {d (1 + e^{- z_i})} {d z_i}<br>\\<br>&amp;= - \dfrac {1} {(1 + e^{- z_i})^2} \times (- e^{- z_i})<br>\\<br>&amp;= \dfrac {1} {(1 + e^{- z_i})^2} \times (e^{- z_i})<br>\\<br>&amp;= \dfrac {1} {1 + e^{- z_i}} \times \dfrac {(1 + e^{- z_i}) - 1} {1 + e^{- z_i}}<br>\\<br>&amp;= \sigma (z_i) \times (1 - \sigma (z_i))<br>\end{aligned}<br>$</p>
<p>② 复合函数内层 $z_i$ 求导：</p>
<p>$<br>\dfrac {\partial z_i} {\partial W} = \dfrac {\partial (W^T X_i)} {\partial W} = X_i<br>$</p>
<p>综合上述 2 式得：</p>
<p>$<br>\begin{aligned}<br>\dfrac {\partial \mu_i} {\partial W} &amp;= \dfrac {d \sigma (z_i)} {d z_i} \dfrac {\partial z_i} {\partial W}<br>\\<br>&amp;= \sigma (z_i) \times (1 - \sigma (z_i)) \times X_i<br>\\<br>&amp;= \mu_i \times (1 - \mu_i) \times X_i<br>\end{aligned}<br>$</p>
<p>回到目标函数梯度，将上式代入求导得：</p>
<p>$<br>\begin{aligned}<br>J_1 (W) &amp;= \sum^N_{i = 1} \left( - y_i \log (\mu (X_i; W)) - (1 - y_i) \log (1 - \mu (X_i; W)) \right)<br>\\<br>&amp;= \sum^N_{i = 1} \left( - y_i \log \mu_i - (1 - y_i) \log (1 - \mu_i) \right)<br>\\<br>&amp;= - \sum^N_{i = 1} \left(y_i \log \mu_i + (1 - y_i) \log (1 - \mu_i) \right)<br>\end{aligned}<br>$</p>
<p>$<br>\begin{aligned}<br>g_1 (W) &amp;= \nabla J_1 (W) = \dfrac {d J_1 (W)} {d W}<br>\\<br>&amp;= - d \left( \sum^N_{i = 1} \left( y_i \log \mu_i + (1 - y_i) \log (1 - \mu_i) \right) \right) / d W<br>\\<br>&amp;= - \sum^N_{i = 1} \left( y_i \dfrac {1} {\mu_i} \times \dfrac {\partial \mu_i} {\partial W} + (1 - y_i) \dfrac {1} {1 - \mu_i} \times - \dfrac {\partial \mu_i} {\partial W} \right)<br>\\<br>&amp;= - \sum^N_{i = 1} \left( y_i \dfrac {1} {\mu_i} - (1 - y_i) \dfrac {1} {1 - \mu_i} \right) \times \mu_i (1 - \mu_i) X_i<br>\\<br>&amp;= - \sum^N_{i = 1} \left( y_i (1 - \mu_i) - (1 - y_i) \mu_i \right) \times X_i<br>\\<br>&amp;= - \sum^N_{i = 1} (y_i - \mu_i) \times X_i<br>\\<br>&amp;= (\mu - y) X<br>\end{aligned}<br>$</p>
<p><strong>整理可得 Logistic 回归损失和部分的梯度表达式：</strong></p>
<p>$$<br>g_1 (W) = X^T (\mu - y)<br>$$</p>
<blockquote>
<p>@注：这里直接解出来的结果是 $(\mu - y) X$，但表达式使用的是 $X^T (\mu - y)$，对此我有些自己的理解方式如下。</p>
</blockquote>
<p>首先要提一下矩阵的形式。在机器学习中有一个很重要的工具包 <code>numpy</code>，这个工具包里其中两个很重要的类：<code>numpy.matrix</code> 和 <code>numpy.array</code>也即矩阵和向量，通常手动创建一个矩阵的时候可以用如下方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = np.matrix([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(A)</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]]</span><br></pre></td></tr></table></figure>

<p>这就可以视为一个 2 x 2 的矩阵：$\left[ \begin{matrix} 1 &amp; 2 \\ 3 &amp; 4 \end{matrix} \right]$，但是当我们如下创建一个向量时：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(B)</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(B.shape)</span><br><span class="line">(<span class="number">2</span>,)</span><br></pre></td></tr></table></figure>

<p><strong>可以看到，尽管输出的 B 的形式还是 <code>[1 2]</code>，但这是一个 2 行（也即 1 列）的向量（之所以没有标出 <code>(2, 1)</code> 是因为向量要么只有 1 行要么只有 1 列，防止与矩阵的 n x m 搞混了）。也就是说，默认情况下的向量是列向量，这也符合机器学习中的直觉，例如标签 y，预测值，或单个特征等都是列向量。</strong></p>
<p><strong>回到原问题的梯度表达式中，假设训练数据共有 D 维特征 N 个样本，则 X 是 N x D 维的矩阵，$\mu$ 和 y 均为 N 行的列向量，都知道矩阵的乘法 (A x B) 需要满足 A 的列数 = B 的行数时才有意义，而当作为矩阵运算 $(\mu - y) X$ 时，$(\mu - y)$ 列数为 1，此时无论 $X$ 是否转置，$(\mu - y) X$ 都无意义，不能做乘法运算。因此 $\sum^N_{i = 1} (\mu_i - y_i) \times X_i$ 转换为矩阵表达式时，将 $X$ 提到左乘并转置，不仅 $X^T$ 的列数恰好为 N，与 $\mu$ 和 $y$ 的行数相等，可以做乘法，而且从矩阵的乘法运算规则上符合直观地计算过程（$X$ 转置后，每一列为一个样本，分别与每个 $(\mu_i - y_i)$ 相乘）。</strong></p>
<p><strong>因此，梯度的表达式为：$g_1 (W) = X^T (\mu - y)$。</strong></p>
<h3 id="3-2-Hessian矩阵"><a href="#3-2-Hessian矩阵" class="headerlink" title="3.2 Hessian矩阵"></a>3.2 Hessian矩阵</h3><p>Logistic 损失和部分的梯度为 $g_1 (W) = X^T (\mu - y)$，由此求解 Hessian 矩阵：</p>
<p>$<br>\begin{aligned}<br>H_1 (W) &amp;= \dfrac {\partial g_1 (W)} {\partial W} = \dfrac {\partial \left( \sum^N_{i = 1} (\mu_i - y_i) X_i \right)} {\partial W} = \dfrac {\partial \left( \sum^N_{i = 1} X^T_i (\mu_i - y_i) \right)} {\partial W}<br>\\<br>&amp;= \sum^N_{i = 1} X^T_i \dfrac {\partial (\mu_i)} {\partial W}<br>\\<br>&amp;= \sum^N_{i = 1} X^T_i \mu_i (1 - \mu_i) X_i<br>\end{aligned}<br>$</p>
<blockquote>
<p>@注：说明：$\dfrac {\partial (a^T y)} {\partial y} = a^T$</p>
</blockquote>
<p><strong>令矩阵 $S \triangleq \mu_i (1 - \mu_i)$，即 $S$ 为对角阵，对角元素为 $\mu_i (1 - \mu_i)$。可得 Hessian 矩阵表达式：</strong></p>
<p>$$<br>H_1 (W) = X^T S X<br>$$</p>
<h3 id="3-3-牛顿法求解Logistic损失函数和极小值：IRLS"><a href="#3-3-牛顿法求解Logistic损失函数和极小值：IRLS" class="headerlink" title="3.3 牛顿法求解Logistic损失函数和极小值：IRLS"></a>3.3 牛顿法求解Logistic损失函数和极小值：IRLS</h3><p>当解得梯度 $g_1 (W) = X^T (\mu - y)$ 和 Hessian 矩阵 $H_1 (W) = X^T S X$ 后，即可代入牛顿迭代公式中：</p>
<p>$<br>\begin{aligned}<br>W^{(t + 1)} &amp;= W^{(t)} - H(W^{(t)})^{-1} g(W)<br>\\<br>&amp;= W^{(t)} - (X^T S^{(t)} X)^{-1} X^T (\mu - y)<br>\\<br>&amp;= (X^T S^{(t)} X)^{-1} (X^T S^{(t)} X) \times W^{(t)} - (X^T S^{(t)} X)^{-1} X^T (\mu - y)<br>\\<br>&amp;= (X^T S^{(t)} X)^{-1} \times \left( X^T S^{(t)} X W^{(t)} - X^T (\mu - y) \right)<br>\\<br>&amp;= (X^T S^{(t)} X)^{-1} \times \left( X^T S^{(t)} X W^{(t)} - X^T S^{(t)} {S^{(t)}}^{-1} (\mu - y) \right)<br>\\<br>&amp;= (X^T S^{(t)} X)^{-1} X^T S^{(t)} \times \left( X W^{(t)} - {S^{(t)}}^{-1} (\mu - y) \right)<br>\end{aligned}<br>$</p>
<p><strong>令 $z^{(t)} \triangleq X W^{(t)} - {S^{(t)}}^{-1} (\mu - y)$，则得到牛顿法迭代求 Logistic 损失函数和极小值的解：</strong></p>
<p>$$<br>W^{(t + 1)} = (X^T S^{(t)} X)^{-1} X^T S^{(t)} z^{(t)}<br>$$</p>
<p>对比线性回归 $X W = y$ 方程中最小二乘的解：$\hat{W}_{OLS} = (X^T X)^{-1} X^T y$，给每个样本加权（每个样本的权重为 $S_i$）即可得加权最小二乘的解：</p>
<p>$<br>\hat{W}_{OLS_weight} = (X^T S X)^{-1} X^T y<br>$</p>
<p>牛顿迭代法求得的 Logistic 损失函数和极小值的解 与 加权最小二乘的解形式类似，因此也称为 <strong>迭代加权最小二乘（Iteratively Reweighted Least Squares, IRLS）</strong>，其中每个样本的权重为 $S_i = \mu_i (1 - \mu_i)$。而 IRLS 又通过 共轭梯度（Conjugate Gradient）法 求解，因此 Scikit-Learn 中采用牛顿法求解的优化算法为 <code>&#39;newton-cg&#39;</code>。</p>
<hr>
<h2 id="4-Logistic的优化求解器Solver"><a href="#4-Logistic的优化求解器Solver" class="headerlink" title="4. Logistic的优化求解器Solver"></a>4. Logistic的优化求解器Solver</h2><p>Logistic 回归有多种优化求解方法。当使用 L2 正则时，可采用所有优化算法，而由于 L1 正则在零点处不可导，因此次不能使用需要计算梯度 / Hessian 矩阵的方法，此时可以类似 Lasso 求解，采用坐标轴下降法。</p>
<p>Scikit-Learn 中的 Logistic 类已在：<a href="/post/machinelearning/ml-logistic/ml-logistic-introduction/ml-logistic-introduction/" title="@LINK">《ML入门——Logistic回归简介》</a> 文中介绍，此处展开参数 <code>solver</code> 的一些可选项：</p>
<p>‘liblinear’：<br>线性求解器，适用于小数据集，支持 L1 正则和 L2 正则。<br>内部使用了坐标轴下降法来迭代优化损失函数，如果模型的特征非常多，希望一些不重要的特征系数归零从而让模型系数稀疏的话，可以使用 L1 正则化。在多类 Logistic 回归任务中仅支持 OvR，不支持多项分布损失（MvM），但 MVM 相对精确。</p>
<p>‘lbfgs’：<br>拟牛顿法，适用于较大数据集，仅支持 L2 正则。<br>支持 OvR 和 MvM 两种多类 Logistic 回归。</p>
<p>‘newton-cg’：<br>牛顿法，适用于较大数据集，仅支持 L2 正则。<br>每个大迭代中的加权最小二乘回归部分采用共轭梯度算法实现。支持 OvR 和 MvM 两种多类 Logistic 回归。</p>
<p>‘sag’：<br>随机平均梯度下降，适用于很大（如大于 5 万）的数据集，仅支持 L2 正则。<br>梯度下降法的变种，支持 OvR 和 MvM 两种多类 Logistic 回归。</p>
<p>‘saga’：<br>改进的随机平均梯度下降，适用于非常大的数据集，支持 L1 正则。<br>当数据量很大，且选择 L1 正则时，只能采用 ‘saga’ 优化求解器。支持 OvR 和 MvM 两种多类 Logistic 回归。</p>
<p>其中，’sag’ 和 ‘saga’ 只有在特征尺度大致相等时才能保证收敛，因此需要对数据做缩放（<code>class sklearn.preprocessing</code> 可以实现如：标准化、MinMaxScaler、MaxAbsScaler 等）。在实际任务中，大部分情况下数据预处理时都最好做标准化。实际上，加正则项本身也要求对每维特征做缩放。</p>
<p>另外，对于大数据集的训练任务，可以使用 <code>SGDClassifier</code>，并使用 LogLoss 作为损失函数。</p>
<blockquote>
<p>@注：若 <code>SGDClassifier</code> 使用 HingeLoss 作为损失函数，则为实现随机梯度下降的 SVM。在回归任务中还有 <code>SDGRegressor</code>。</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">




<script>
    <!--动态浏览器标签-->
    var OriginTitle = document.title;
    var titleTime;
    var titleTime2;
    var titleTime3;
    var blankTitle = "\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000"
    blankTitle = blankTitle + blankTitle + blankTitle;
    document.addEventListener('visibilitychange', function () {
        if (document.hidden) {
            $('[rel="icon"]').attr('href', "/images/favicon-32x32-next.png");
            // 失去焦点时不切换标题
            // document.title = "Waiting...";
            document.title = OriginTitle;
            clearTimeout(titleTime);
            clearTimeout(titleTime2);
            clearTimeout(titleTime3);
        }
        else {
            document.title = "Welcome";
            titleTime = setTimeout(function () {
                document.title = "Back";
                titleTime2 = setTimeout(function () {
                    document.title = "Welcome Back !";
                    titleTime3 = setTimeout(function () {
                        document.title = OriginTitle;
                    }, 500);
                }, 300);
            }, 300);
        }
    });
</script>






    
    
    <style type="text/css">
        .postEndContainer {
            border-radius: 10px;
            margin-top: 50px;
            margin-bottom: 10px;
        }
        .toAbout {
            font-size: 18px;
            text-align: center;
            color: #EB6D39;
            cursor: pointer;
            border-bottom: none;
        }
        .toAbout:hover {
            background: #EB6D39;
            color: #FFFFFF;
            cursor: pointer;
            border-bottom: none;
        }

        .postEndText {
            animation: slideHorizontal 4.0s ease infinite;
        }
        
        @keyframes slideHorizontal {
            0% {
                transform: rotateX(0deg);
            }
            25% {
                transform: rotateX(360deg);
            }
            50% {
                transform: rotateX(720deg);
            }
            100% {
                transform: rotateX(720deg);
            }
        }
    </style>
    
        
        <div class="pagination postEndContainer toAbout" onClick="window.open('/about/','_self')">
            <div class="postEndText">全 文 结 束&ensp;&ensp;<i class="fab fa-leanpub"></i>&ensp;&ensp;点 击 留 言</div>
        </div>
    

    
    <img src="/images/reward.png" style="width: 300px; max-width: 100%;">


          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者: </strong>Luis
  </li>
  <li class="post-copyright-link">
      <strong>本文链接:</strong>
      <a href="https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/" title="ML入门-Logistic优化求解与牛顿法">https://www.liushulun.cn/post/machinelearning/ml-logistic/ml-logistic-optimization/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明: </strong>所有文章均由 Luis 原创于 liushulun.cn，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议，引用前请务必联系授权。
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"><i class="fa fa-tag"></i> AI</a>
              <a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/Logistic%E5%9B%9E%E5%BD%92/" rel="tag"><i class="fa fa-tag"></i> Logistic回归</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/machinelearning/ml-logistic/ml-logistic-multiclassify/" rel="prev" title="ML入门-多类分类任务">
                  <i class="fa fa-chevron-left"></i> ML入门-多类分类任务
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/machinelearning/ml-logistic/ml-logistic-introduction/" rel="next" title="ML入门-Logistic回归简介">
                  ML入门-Logistic回归简介 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuShulun</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Site Symbols:</span>
    <span title="Site Symbols">355k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Site Read Time &asymp;</span>
    <span title="Site Read Time">6:35</span>
  </span>
</div>


    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>





  <script>
    NProgress.configure({
      showSpinner: true
    });
    NProgress.start();
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'interactive') {
        NProgress.inc(0.8);
      }
      if (document.readyState === 'complete') {
        NProgress.done();
      }
    });
    document.addEventListener('pjax:send', () => {
      NProgress.start();
    });
    document.addEventListener('pjax:success', () => {
      NProgress.done();
    });
  </script>

  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>




<style type="text/css">
    .toBottom {
        color: #EEEEEE;
        border-bottom: none;
    }
    .toBottom:hover {
        color: #EB6D39;
        border-bottom: none;
    }
</style>
<div class="scrollToBottom back-to-top back-to-top-on" role="button" style="bottom: 30px;" onClick="window.open('#postBottom','_self')">
    <span class="fa fa-arrow-down"> Down</span>
</div>

<script>
    $(function() {
        $('.scrollToBottom').click(function(){
            
            $('html,body').animate({scrollTop:$('.bottom').offset().top}, 800);
        });
    })
</script>

<a id='postBottom'></a>
</body>
</html>
